# CICD implementation of the 3 microservices project onto and EKS cluster and onto a non-EKS cluster using GitLab with properly authenticated user gitlab for kops (non-EKS) and IAM user gitlab2 for EKS for provisioning

## Tools
GitLab

AWS EKS (using Cloudformation stack) and IAM user role provisioning.  For this we need to supply gitlab with the IAM AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY as well as the kubeconfig file contents in the ENV vars for the project. Provisioning will be through the gitlab2 IAM user 

non-EKS implementation will use kops and create the CA cert, public cert and private cert so that the kube config file can be created to access the cluster. For this, gitlab needs to be supplied with the kubeconfig contents in the ENV vars for the project.

In both setups a cicd role is created and edited (ConfigMap) for proper permissions.  The cicd role is then bound to the gitlab2 user (EKS) or gitlab user (Non-EKS). For EKS the IAM gitlab2 user credentials need to be added to the ENV vars of gitlab.  This is how gitlab gets access to the cluster

For application deployment use Helm.   This will be an 3 tier application deployment with Golang, javascript and python code for each of the respective microservices

Docker: For the microservices themselves, docker is used. This is development tested prior to this project and docker-compose was ultimately used to wrap up the pre-development testing of the microservices on the EC2 controller.  Then helm (charts) wasintroduced into source code so that the project could be easily deployed to a kops K8s cluster.   

Docker hub repos for each of the 3 microservices. Docker hub is used as the container registry/repo (similar to ECR on AWS) for the PUSH stage (task) of the gitlab-ci.yml CI/CD pipeline manifest file.  Images are SHA1 tagged so that they can be used in subsequent pipeline stages (DELIVER and DEPLOY)

## Github and gitlab repo design

In order to facilitate the transition of the setup to gitlab the following design was employed for the local VSCode workspace on mac and the EC2 controller workspace to exchange code updates

VSCode is mapped to old-origin repo on github for this CI/CD project as well as origin repo on gitlab
EC2 controller workspace is maapped to both github (as old-origin) and gitlab repo (as origin)

The gitlab repo is basically the gitlab runner code. The gitlab runner runs all of the .gitlab-ci.yml stages  in accordance to the software branch being employed and deployed.(staging and production on staging and default namespace)

The flow **for main branch** is to commit a change on VSCode and push to github
Then pull the code from github onto the EC2 controller
Then push the code from EC2 controller to the gitlab repo to start the pipeline

The controller should not be committing to main gitlab repo directly in ordinary software cycle
Instead for feature change create branch feature on VSCode and push the change to github
Pull the feature change on feature branch to the controller, from github
Then push the feature change from the EC controller on feature branch to the gitlab repo 
This will run a build stage only (non-main branch push) just to insure that the docker image can be built without errors (see below for more information on the gitlab pipeline stages)

The gitlab repo will only deploy docker image for main branch push.  For other branches there is simply a docker image build test (this will be done on the feature branch commit change)

Once the feature change is pushed to gitlab on feature branch, Create a merge request for merging the feature change into main. This is done on gitlab (gitlab repo). This will cause the build stage to run to verify the merge build integrity.  The gitlab CI/CD settings are such that this must PASS in order to actually perform the merge of feature into main branch.

Once this merge request build is passed, the actual Merge can be approved into main  This is a commit into main and so the build stage will be run and then the push stage to docker hub for tagged image, and then fnally the deliver stage to the cluster using helm with the merged feature to main change (staging namespace on the k8s cluster). At this point the new feature in the fully running application can be tested and if approved promoted to  production deployment and then deployed to production (deploy stage) (more on this below)


## Synchronizing of repos:

After the feature change is muerged to main, pull changes to main on the EC2 controller workspace to keep it up to date
Also pull the changes to main on the local VSCode to keep it up to date
At this point the VSCode can do a push to github repo to keep it up to date.


## .gitlab-ci.yml. This file dictates the actions performed during the pipelines. This file must be placed in the root of the gitab repo. All paths in the yml file to source code for docker building, and helm deployment (location of helm charts for the resepctive microservices) have to be done relative to this location.

stages:
BUILD
PUSH
DELIVER
PROMOTE
DEPLOY

The .gitlab-ci.yml file dictates the jobs(stages) that are run on gitlab to facilitate the pipelines:

For build stage this is for all branches and simply builds the docker image test

For the push stage to docker hub, this is only done with the final version, i.e. push to main. All of these images wll have SHA1 tags so that they can be used in subsequent stages (below)

For the deliver stage to the EKS cluster using helm, and namespace staging, the deliver stage does this to the active EKS cluser (using the gilab2 credential; see above)

For the promote and deploy to production (default namespace) the promote is only performed on a tagged version and once promoted the manual decision to deploy it to the production cluster (namespace default)


## EKS implementation with eksctl (EKS K8s cluster) with gitlab pipelines

### Step1: Bring up the EC2 controller

The controller has all neceassary software for executing the CI/CD pipeline.  eksctl and kops are installed

### Step2: On terminal1 SSH to EC2 controller a

cd into the following directory on the controller

~/course3_eksctl

### Step3: export KUBECONFIG

export the KUBEONFIG env variable on the terminal session
export KUBECONFIG=~/.kube/eksctl/config

### Step4: Run eksctl create cluster

from the ~/course3_eksctl directory execute:
eksctl create cluster -f cluster.yaml

The current size is t3.large. I can reduce this down to t3.small similar to what is being used on kops since high load is not expected.  Run a test to ensure that cluster can run with the reduced node size.  The weatherapp will eventually have to run on the cluster via helm deployment fron gitlab runner.


### Step5: On separate SSH terminal on the controller: Create the permissions policy and role cicd and bind to gitlab2 (this is done on staging namespace and default namespace; as noted above the default namespace will be used for promition deploy to the production cluster)

This is so that gitlab can provision the cluster as required by many of the stages above in the .gitlab-cy.yml file


Download the k8s ConfigMap template onto the controller

curl -o aws-auth-cm.yaml https://s3.us-west-2.amazonaws.com/amazon-eks/cloudformation/2020-10-29/aws-auth-cm.yaml

ubuntu@ip-172-31-21-52:~/course3_eksctl$ ls -la
total 59344
drwxrwxr-x  6 ubuntu ubuntu     4096 Jul  1 19:11 .
drwxr-x--- 26 ubuntu ubuntu     4096 Jul  1 05:54 ..
drwxrwxr-x  8 ubuntu ubuntu     4096 Jul  1 19:11 .git
-rw-rw-r--  1 ubuntu ubuntu       14 Jun 24 22:12 .gitignore
-rw-rw-r--  1 ubuntu ubuntu      538 Jun 20 22:32 01-simple-cluster.yaml
-rw-rw-r--  1 ubuntu ubuntu    11776 Jun 26 02:10 README
drwxr-xr-x  3 ubuntu ubuntu     4096 Jun 20 18:16 aws
-rw-rw-r--  1 ubuntu ubuntu      450 Jul  1 00:10 aws-auth-cm.yaml   <<<<<<<<<<<<<<<<<<<
-rw-rw-r--  1 ubuntu ubuntu      447 Jun 25 01:11 aws-auth-cm_OLD.yaml
-rw-rw-r--  1 ubuntu ubuntu 60707665 Jun 24 22:11 awscliv2.zip
-rw-rw-r--  1 ubuntu ubuntu      296 Jul  1 19:11 cluster.yaml
drwxrwxr-x  2 ubuntu ubuntu     4096 Jun 26 00:10 export_KUBECONFIG_eksctl_config
drwxrwxr-x  3 ubuntu ubuntu     4096 Jun 20 22:32 sample_yaml_files



Run aws sts get-caller-identity to get the account id number



Edit the aws-auth-cm.yaml file with the following


apiVersion: v1
kind: ConfigMap
metadata:
  name: aws-auth
  namespace: kube-system
data:
  mapRoles: |
    - rolearn: arn:aws:iam::590183769797:role/eksctl-cluster2-nodegroup-ng-1-NodeInstanceRole-N<instance-role-id>
      username: system:node:{{EC2PrivateDNSName}}
      groups:
        - system:bootstrappers
        - system:nodes
  mapUsers: |
    - userarn: arn:aws:iam::<service-account-number>:user/gitlab2
      username: gitlab2
      groups:
        - cicd   <<<< note that this is the role that will be creatd for the gitlab2 user

<servcie-account-number> is from command above
The cluster <instance-role-id> is from the AWS console. It is under the Cloudformation stack ----> ndoegroup link ---> Resources tab ----> NodeInstanceRole link.  This hyperlinks to the IAM role for the cluster nodes
The id is the trailing digits on the ARN

Once edited save the file above

Apply the ConfigMap to the cluster:  kubectl apply -f aws-auth-cm.yaml

ubuntu@ip-172-31-21-52:~/course3_eksctl$ kubectl apply -f aws-auth-cm.yaml
configmap/aws-auth created


Verify the contents of the config map:  kubectl describe configmap -n kube-system aws-auth

The ConfigMap can be edited with the following command:  kubectl edit cm/aws-auth -n kube-system


Next, create namespace staging on the cluster
kubectl create namespace staging

Default ns will be used for production deployment (see repo/pipeline design notes above)

ubuntu@ip-172-31-21-52:~/course3_eksctl$ kubectl create ns staging
namespace/staging created
ubuntu@ip-172-31-21-52:~/course3_eksctl$ kubectl get ns
NAME              STATUS   AGE
default           Active   48m
kube-node-lease   Active   48m
kube-public       Active   48m
kube-system       Active   48m
staging           Active   5s




Create role cicd in staging ns:   kubectl create role cicd --verb="*" --resource="*" --namespace staging

ubuntu@ip-172-31-21-52:~/course3_eksctl$ kubectl create role cicd --verb="*" --resource="*" --namespace staging
role.rbac.authorization.k8s.io/cicd created


Edit apiGroups in the role cicd in staging ns: kubectl edit role cicd --namespace staging


apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  creationTimestamp: "2024-07-01T20:44:20Z"
  name: cicd
  namespace: staging
  resourceVersion: "8715"
  uid: ef3e537c-e618-4513-82b5-96986b010c06
rules:
- apiGroups:
  - ""      <<<<<<<< put * here 
  resources:
  - '*'
  verbs:
  - '*'



Finally bind the role cicd with the user gitlab2: kubectl create rolebinding cicd --role cicd --user=gitlab2 -n staging

ubuntu@ip-172-31-21-52:~/course3_eksctl$ kubectl create rolebinding cicd --role cicd --user=gitlab2 -n staging
rolebinding.rbac.authorization.k8s.io/cicd created



Do the same for the default ns (prodution):
kubectl create role cicd --verb="*" --resource="*" 
kubectl edit role cicd
kubectl create rolebinding cicd --role cicd --user=gitlab2



### Step6: Test out the role provisioning of the gitlab2 user:

switch to AWS_PROFILE=gitlab2 on the SSH terminal
Verify with: aws configure list

run kubectl get pods -n staging and kubectl get pods
Both should be successful

Run kubectl get pods -n kube-system.  This should fail
Run kubectl get nodes This should fail as well.

ubuntu@ip-172-31-21-52:~/course3_eksctl$ kubectl get pods
No resources found in default namespace.
ubuntu@ip-172-31-21-52:~/course3_eksctl$ kubectl get pods -n staging
No resources found in staging namespace.
ubuntu@ip-172-31-21-52:~/course3_eksctl$ kubectl get pods -n kube-system
Error from server (Forbidden): pods is forbidden: User "gitlab2" cannot list resource "pods" in API group "" in the namespace "kube-system"
ubuntu@ip-172-31-21-52:~/course3_eksctl$ kubectl get node
Error from server (Forbidden): nodes is forbidden: User "gitlab2" cannot list resource "nodes" in API group "" at the cluster scope






### Step7: Update the gitlab ENV variable for the latest EKS .kube config file. The ENV variable on gitlab is called KUBECONFIG_EKS


In the controller SSH terminal 
cat ~/.kube/eksctl/config | base64 | tr -d '\n' && echo

And paste the contents into the ENV variable KUBEOCNFIG_EKS on gitlab Settings ---> CICD ----> Variables

Gitlab will use this kubecocnfig to get access to the current EKS with the permissions/role specified for gitlab2 user as indicated in previous section above




## Test this out with a feature commit (EKS cluster with gitlab pipelines)

Create a new branch each time a new feature or software chage.
Always create the new branch from MAIN because the full README is in MAIN and the feature branch will be merged back into MAIN at the gitlab repo. Otherwise the README could get overwritten.

### First create the feature-1 branch on the EC2 controller.  

git checkout -b feature-1

### Modify the code for the feature branch change


ubuntu@ip-172-31-21-52:~/course3_projects/4_CICD_EKS_Gitlab$ git checkout feature-1 
M       .gitignore
Switched to branch 'feature-1'
ubuntu@ip-172-31-21-52:~/course3_projects/4_CICD_EKS_Gitlab$ ls
4_cicd_yml  README  base64  source_files  test.yaml
ubuntu@ip-172-31-21-52:~/course3_projects/4_CICD_EKS_Gitlab$ cd source_files/
ubuntu@ip-172-31-21-52:~/course3_projects/4_CICD_EKS_Gitlab/source_files$ ls
weatherapp
ubuntu@ip-172-31-21-52:~/course3_projects/4_CICD_EKS_Gitlab/source_files$ cd weatherapp/
ubuntu@ip-172-31-21-52:~/course3_projects/4_CICD_EKS_Gitlab/source_files/weatherapp$ ls
ORIGINAL_SOURCE_helm_charts  auth     docker-compose.yaml    gitlab_kops_cert_user1  weatherapp-auth  weatherapp-weather
UI                           db-data  gitlab_kops_cert_user  weather                 weatherapp-ui
ubuntu@ip-172-31-21-52:~/course3_projects/4_CICD_EKS_Gitlab/source_files/weatherapp$ cd UI
ubuntu@ip-172-31-21-52:~/course3_projects/4_CICD_EKS_Gitlab/source_files/weatherapp/UI$ ls
Dockerfile  app.js  package-lock.json  package.json  public
ubuntu@ip-172-31-21-52:~/course3_projects/4_CICD_EKS_Gitlab/source_files/weatherapp/UI$ cd public/
ubuntu@ip-172-31-21-52:~/course3_projects/4_CICD_EKS_Gitlab/source_files/weatherapp/UI/public$ ls
index.html  login.html  signup.html  static
ubuntu@ip-172-31-21-52:~/course3_projects/4_CICD_EKS_Gitlab/source_files/weatherapp/UI/public$ vi login.html 



edit login.html "Please log in test2" to Please log in test3"

<div class="container-fluid">
    <form class="form-signin col-xs-8 col-xs-offset-2  col-sm-6 col-sm-offset-3 col-md-4 col-sm-offset-4 col-lg-2 col-lg-offset-5" method="post" action="/login">
        <h2 class="form-signin-heading">Please log in test3</h2>


### Commit the changes to the feature-1 branch



### Push the changes to the gitlab repo on feature-1 branch. This will run the build stage only pipeline.
This will test the integrity of the docker images with the change



### Merge request assuming the build state passes

NOTE: a link is created by gitlab to hypelink directly to the merge request page when pushing the original feature branch change to gitlab (see above)
The link is below.


ubuntu@ip-172-31-21-52:~/course3_projects/4_CICD_EKS_Gitlab/source_files/weatherapp$ git push origin feature-1 
Enumerating objects: 15, done.
Counting objects: 100% (15/15), done.
Delta compression using up to 2 threads
Compressing objects: 100% (8/8), done.
Writing objects: 100% (8/8), 655 bytes | 655.00 KiB/s, done.
Total 8 (delta 6), reused 0 (delta 0), pack-reused 0
remote: 
remote: To create a merge request for feature-1, visit:
remote:   https://gitlab.com/dmastrop/course3_gitlab_section4_weaterapp/-/merge_requests/new?merge_request%5Bsource_branch%5D=feature-1
remote: 
To https://gitlab.com/dmastrop/course3_gitlab_section4_weaterapp.git
 * [new branch]      feature-1 -> feature-1



### Create the MR

This runs another pipeline build stage (only) to test the merged code into main (but the code has not been committed to main at this point)

The actual Merge will be in blocked state. This will be in a blocked state for the actual merge until this build pipeline is run on the merge code. Once this passes the code can actually be merged into main


### Merge the code into main branch


This will actually run 3 stages in the pipeline: build again, then push (push the main merged code to docker hub with tag) and finally deliver stage, where the main code is pushed to the staging environment.  This is where the code can actually be functionally tested with the feature change in place.  Helm is used to deploy the code in the .gitlab-ci.yml deliver stage.


The pods in the staging namespace should all be started and running and the code can be functionally tested.  The deployment of the application is done through helm.

There is a current issue with the backend mysql which is causing the Crash back off on the authentication microservice

ubuntu@ip-172-31-21-52:~/course3_eksctl$ kubectl get pods -n staging
NAME                                  READY   STATUS             RESTARTS      AGE
weatherapp-auth-6ff98cf84d-ssxj2      0/1     CrashLoopBackOff   2 (23s ago)   47s
weatherapp-auth-6ff98cf84d-xqjth      0/1     CrashLoopBackOff   2 (22s ago)   47s
weatherapp-auth-mysql-0               0/1     Pending            0             47s
weatherapp-ui-d6dfff549-cf5k5         1/1     Running            0             46s
weatherapp-ui-d6dfff549-dj2tl         1/1     Running            0             46s
weatherapp-weather-7c556b64bd-4cz2n   1/1     Running            0             45s
weatherapp-weather-7c556b64bd-jf9lc   1/1     Running            0             45s




## Once complete synch up the other repos

NOTE: the feature branch is deleted along with the Merge request/Merge on gitlab repo. So the only branch left will be main. This is by design as the feature branches are only meant to be ephemeral for a feature commit.

Once the code is delivered to staging, synch up the EC2 controller local repo and the VSCode mac local repo.

Pull the gitlab main repo to their respective local repos

git pull origin main

Once VSCode is synched, push the main branch to github repo (old-origin; origin is used for gitlab repo and old-origin is used for github repo).

NOTE: no need to synch up the feature branches as they are ephmeral and they are removed on gitlab.




## The next stages in the gitlab pipeline are promote and deploy to production. The k8s namespace default is used for the production dpeloyment.


The code in the .gitlab.cy.yml file is below

The approach uses git tagging to select commit bulds for promotion to production deployment.
The git log can be run on the EC2 controller and selected builds based upon commit can be tagged, for example 1.0.1 and 1.0.2. Note that these tagged builds are differentiated from the COMMIT SHA1 tagged bulds on docker hub and for scripting purposes.  The git tagged bulds use the CI_COMMIT_TAG env variable which is used to create the TAGGED_HASH=`git rev-list -n 1 $CI_COMMIT_TAG, whereas the hash commit tagged bulds use the CI_COMMIT_SHA (see push stage in the gitlab-cy.yml script file).  The CI_COMMIT_TAG is useful for the rules setting in each task/job so that we only run promote and deploy stages with CI_COMMIT_TAG docker images on docker hub.

The selected images based upon git log are tagged with: git rev-list -n 1 $CI_COMMIT_TAG for example
git rev-list -n 1 1.0.1

The methodoligy for candidacy promotion is to first pull the TAGGED_HASH correpsonding build (based on CI_COMMIT_TAG) from docker hub to the gitlab runner running the script. The runner can then tag the image with the CI_COMMIT_TAG and then push the image back to docker hub. These are the CI_COMMIT_TAG images that are present on docker hub 

The images can then be selected for promotion with a feature commit and merge to main. The merge to main 
For each commit there is an explicit git tag that follows up on the EC2 controller, for example
git tag -a 1.0.1 -m “login.html feature change”

This is then pushed from the EC2 controller to the gitlab repo:
git push --tags origin main

The push of the tags cause another pipeline to run but only on promoate state. The pipeline then goes into BLOCKED state, at which time there has to be a manual decision to deploy the tagged build to the default namespace production k8s cluster.   For multiple pushed tag images any one of them can be selected for promotion to deployment.  

Run pipeline can be run from gitlab to select them directly.

Each time a new deployment is made to default namespace the kubectl get pods can be verified for pod rotation/replacment and also the kubectl get svc LoadBalancer can be used to verify the actual change on the browser (or using Selenium for a more automated approach to the QA)

kubectl describe pod <pod name> will show the tagged version in the details as well.


### PROMOTE and DEPLOY blocks of .gitlab-cy.yml with comments and notes



promote:
  stage: promote
  services:
    - docker:dind
  image: docker:latest
  before_script:
    - apk add git
    - docker login -u "$CI_REGISTRY_USER" -p "$CI_REGISTRY_PASSWORD"

    # need to get the commit hash that corresponds to the tag that we will use
    # For example: git rev-list -n 1 1.0.0 
    # will give the commit hash in the git log that has this 1.0.0 version
    # This will be reffered to as the TAGGED_HASH.   CI_COMMIT_TAG for example is the 1.0.0 assigned tag to the commit
    # The TAGGED_HASH are the docker images already pushed to docker hub

    - TAGGED_HASH=`git rev-list -n 1 $CI_COMMIT_TAG`
  script:
    - echo "Promoting Docker images"
    # pull the commit tagged TAGGED_HASH image that is tagged with the CI_COMMIT_TAG, for example 1.0.0
    - docker pull $CI_REGISTRY_USER/weatherapp-ui:$TAGGED_HASH
    - docker pull $CI_REGISTRY_USER/weatherapp-auth:$TAGGED_HASH
    - docker pull $CI_REGISTRY_USER/weatherapp-weather:$TAGGED_HASH
    # Next tag the TAGGED_HASH version back but as the CI_COMMIT_TAG. Thus we have a 1.0.0. docker tagged image now for promotion candidate
    - docker tag $CI_REGISTRY_USER/weatherapp-ui:$TAGGED_HASH $CI_REGISTRY_USER/weatherapp-ui:$CI_COMMIT_TAG
    - docker tag $CI_REGISTRY_USER/weatherapp-auth:$TAGGED_HASH $CI_REGISTRY_USER/weatherapp-auth:$CI_COMMIT_TAG
    - docker tag $CI_REGISTRY_USER/weatherapp-weather:$TAGGED_HASH $CI_REGISTRY_USER/weatherapp-weather:$CI_COMMIT_TAG
    # Push these CI_COMMIT_TAG, for example 1.0.0 images to docker hub so that they can be accessed for promotion to deployment
    - docker push $CI_REGISTRY_USER/weatherapp-ui:$CI_COMMIT_TAG
    - docker push $CI_REGISTRY_USER/weatherapp-auth:$CI_COMMIT_TAG
    - docker push $CI_REGISTRY_USER/weatherapp-weather:$CI_COMMIT_TAG
  rules:
     # any image that has been git tagged as above with git rev-list -n 1 $CI_COMMIT_TAG will get this promotion candidacy and push to docker hub for possible
     # deployment
     # ALL git tagged images will be promoted and are candidates to be manually promoted to production (final stage below)
    - if: '$CI_COMMIT_TAG'





deploy:
    # The deploy stage is based on many approvals. This is deployment to production (k8s namespace default)
    # Thus it will be manual and not auto-triggered based upon a branch push.

  stage: deploy
  image:
    name: alpine/helm:latest
    entrypoint: [""]

  before_script:
    - apk add py3-pip

    ## This pip3 installation method is now longer working
    #- pip3 install awscli
    # For non-EKS cluster do not need this awscli installed. This is only for EKS cluster (which is what I am using)

    ## try using the method below with apk
    # install dependencies
    - apk add python3 py3-awscrt py3-certifi py3-colorama py3-cryptography py3-dateutil py3-distro py3-docutils py3-jmespath py3-prompt_toolkit py3-ruamel.yaml py3-urllib3
    # install awscli
    #- apk add aws-cli –repository=https://dl-cdn.alpinelinux.org/alpine/edge/community
    - apk add aws-cli

    ## try this
    #- pip install urllib3
    #- apk add python3 py3-awscrt py3-certifi py3-colorama py3-cryptography py3-dateutil py3-distro py3-docutils py3-jmespath py3-prompt_toolkit py3-ruamel.yaml
    #- apk add aws-cli –repository=https://dl-cdn.alpinelinux.org/alpine/edge/community/x86_64

    # ## ALTERNATE SOLUTION is to use a virtual environment.
    # - python3 -m venv /path/to/venv
    # # activate the venv
    # - . /path/to/venv/bin/activate
    # # install awscli within this virtual environment
    # - pip install awscli
    # # exit the venv (note this does not remove it' it can be reactivated with the activate command above)
    # - deactivate


    #- aws configure set region "eu-west-2"
    - aws configure set region "us-east-1"
    - mkdir /tmp/.kube
    #- echo $K8S-CONFIG | base64 -d > /tmp/.kube/config
    - echo $KUBECONFIG_EKS | base64 -d > /tmp/.kube/config
    # helm needs access to kubeconfig to provision the app on the EKS cluster
    # The K8S-CONFIG env variable
    - helm repo add bitnami https://charts.bitnami.com/bitnami
    # this is required for mysql dependency helm chart
  
  script:
    # access the respective helm charts for the microservices app deployments similar to deliver stage above
    # This stage is similar to deliver stage with 2 differences: Use CI_COMMIT_TAG in helm deployment. These are promoted images in promote stage above
    # Second difference is deployment is to default namespcace on K8s cluster and not the staging namespace.
    # ALTERNATIVELY we can deploy to a different k8s cluster if required by changing the --kubeconfig argument below.
    #- cd weatherapp-auth
    - cd source_files/weatherapp/weatherapp-auth
    - helm dependency build .
    - helm upgrade --install weatherapp-auth --kubeconfig /tmp/.kube/config --set mysql.auth.rootPassword=$DB_PASSWORD --set image.tag=$CI_COMMIT_TAG .
    - cd ../weatherapp-ui
    - helm upgrade --install --kubeconfig /tmp/.kube/config weatherapp-ui --set image.tag=$CI_COMMIT_TAG .
    - cd ../weatherapp-weather
    - helm upgrade --install --kubeconfig /tmp/.kube/config weatherapp-weather --set apikey=$API_KEY --set image.tag=$CI_COMMIT_TAG .
  rules:
    # only promoted builds with CI_COMMIT_TAG (git tagged) are candidates for promotion to production deployment.
    - if: '$CI_COMMIT_TAG'
    # see above. It must be manual due to approval process. User has to presss a button to deploy to production.
      when: manual












## Once complete synch up the other repos

Do a git pull on the EC2 controller fomr origin/main to synch up the gitlab merged feature changes on main back to the EC2 local repo. Do the same on local VSCode workspace on the mac.  Finally push the code from VSCode to github.   Also pull the tags from gitlab main to EC2 controller and to VSCode and push the tags from VSCode to github.




















## to delete the cluster (There is a known bug so disable nodegroup eviction)

eksctl delete cluster -f cluster.yaml --disable-nodegroup-eviction


eksctl delete cluster --disable-nodegroup-eviction or eksctl delete nodegroup --disable-eviction.


test
